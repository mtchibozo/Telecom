{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab : Nonnenegative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import svds\n",
    "import scipy as sp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in all directories\n",
    "def build_M_matrix(stop):\n",
    "    \n",
    "    M = []\n",
    "    \n",
    "    i = 0\n",
    "    for element in os.listdir('att_faces'):\n",
    "        if element != 'README':\n",
    "            for image in os.listdir('att_faces/' + element):\n",
    "                image_name = 'att_faces/' + element + '/' + image\n",
    "                M.append(np.ravel(plt.imread(image_name)))\n",
    "                i += 1\n",
    "                if stop and i==10:\n",
    "                    return M\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = build_M_matrix(False)\n",
    "M = np.array(M)\n",
    "M = M.astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the database = 400\n",
      "Number of pixels in a image = 10304\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in the database = ' + str(len(M)))\n",
    "print('Number of pixels in a image = ' + str(M[0].size))\n",
    "#We suppose here that all images have the same number of pixels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that if a function $f:\\mathbb{R}^{n}\\longrightarrow\\mathbb{R}$ is convex, then $f:\\mathbb{R}^{2}\\longrightarrow f(0,...,0,x,0,...,0,y,0,...0)$ is also convex.\n",
    "\n",
    "\n",
    "Let's prove that the function isn't convex by contraposition.\n",
    "\n",
    "Let $f:(W_{1,1},...,W_{1,k},...,W_{n,k},H_{1,1},...,H_{1,p},...,H_{k,p})\\longrightarrow \\dfrac{1}{2np}\\sum_{i=1}^{n}\\sum_{l=1}^{p}(M_{i,l}-\\sum_{j=1}^{k}W_{i,j}H_{j,l})^2$\n",
    "\n",
    "Let us show that  $f:(x, y)\\longrightarrow (1-xy)^2$ is not convex.\n",
    "If we take for instance the three couples $(-1,-1)$ and $(1,1)$ an $(0,0)$.\n",
    "We have $f(-1,-1) = 0$ , $f(1,1) = 0$ , $f(0,0) = 1$  \n",
    "With $t = \\dfrac{1}{2}$, we have $f(0,0) > \\dfrac{1}{2}f(1,1) + \\dfrac{1}{2}f(-1,-1)$  \n",
    "That shows that $f:(x, y)\\longrightarrow (1-xy)^2$ is not convex.\n",
    "\n",
    "If we take W and H non negative matrices with only one coefficent x or y, we get the previous function we just studied.\n",
    "That shows that $f:(W_{1,1},...,W_{1,k},...,W_{n,k},H_{1,1},...,H_{1,p},...,H_{k,p})\\longrightarrow \\dfrac{1}{2np}\\sum_{i=1}^{n}\\sum_{l=1}^{p}(M_{i,l}-\\sum_{j=1}^{k}W_{i,j}H_{j,l})^2$ is not convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f:(W_{1,1},...,W_{1,k},...,W_{n,k},H_{1,1},...,H_{1,p},...,H_{k,p})\\longrightarrow \\dfrac{1}{2np}\\sum_{i=1}^{n}\\sum_{l=1}^{p}(M_{i,l}-\\sum_{j=1}^{k}W_{i,j}H_{j,l})^2$\n",
    "\n",
    "We know by definition :\n",
    "\n",
    "$f(W+w, H+h) = f(W,H) + <\\nabla_{W}f(W, H), W> + <\\nabla_{H}f(W, H), h> + o(\\|wh\\|)$\n",
    "\n",
    "Let's calculate it in another way.\n",
    "\n",
    "$f(W+w, H+h) = \\dfrac{1}{2np}\\|M-(W+w)(H+h)\\|^{2}$\n",
    "\n",
    "We develop the scalar product to have :\n",
    "\n",
    "$f(W+w, H+h) = \\dfrac{1}{2np}(<M-WH, M-WH>-2<M-WH, Wh>-2<M-WH, wH>) + o(\\|wh\\|)$\n",
    "\n",
    "$f(W+w, H+h) = \\dfrac{1}{2np}(f(W,H)-2<M-WH, Wh>-2<M-WH, wH>) + o(\\|wh\\|)$\n",
    "\n",
    "We multiply by the adjoint\n",
    "\n",
    "$f(W+w, H+h) = \\dfrac{1}{2np}(f(W,H)-2<W^{T}(M-WH), h>-2<(M-WH)H^{T}, w>) + o(\\|wh\\|)$\n",
    "\n",
    "We can identify the gradient :\n",
    "\n",
    "$\\nabla_{H}f(W, H) = \\dfrac{-1}{np}W^{T}(M-WH)$\n",
    "\n",
    "$\\nabla_{W}f(W, H) = \\dfrac{-1}{np}(M-WH)H^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proving that the function is Lipschitz continuous is equivalent to saying that the function is bounded by a linear function. The degree of both gradient is 3. It can't be bounded by a linear function, so the function isn't Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find W when H0 is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the singular decomposition value. The Matrix $S$ is diagonal and contains the square root of all positive eigenvalues of $S$. It seems relevent to take $\\sqrt{S}$\n",
    "\n",
    "We want a nonnenegative factorization of $M$ as $M \\approx WH$. Starting with the SVD is a good start as it gives a good approximation of the factorization. Besides, python algorithm for SVD is fast.\n",
    "\n",
    "We have $M \\approx W_{0}SH_{0}$\n",
    "\n",
    "$M \\approx (W_{0}\\sqrt{S})(\\sqrt{S}H_{0})$\n",
    "\n",
    "As we want a factorization of M as the product of two matrices, we can write :\n",
    "\n",
    "$M \\approx (W_{0}S\\sqrt{H_{0}})(\\sqrt{H_{0}})$ or $ M \\approx (\\sqrt{W_{0}})(\\sqrt{W_{0}}SH_{0})$ but we have to be sure that we have the right to take the quare root of $W_{0}$ and $H_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that g is convex, we use the same method (question 2.1):\n",
    "\n",
    "Here we take the function : $f:x\\longrightarrow(1-\\alpha^{2}) \\forall \\alpha \\in \\mathbb{R}$. We see that this function is convex. As $g(W) = \\dfrac{1}{2np}\\|M-WH_{0}\\|_{F}^{2}$ we can directly say that g is convex.\n",
    "\n",
    "\n",
    "\n",
    "$\\nabla g = \\nabla _{H} f = \\dfrac{-1}{np}(M-WH)H^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(W, H):\n",
    "    return np.concatenate((W.ravel(), H.ravel()))\n",
    "\n",
    "def unvectorize_M(W_H, M):\n",
    "    # number of elements in W_H is (n+p)*k where M is of size n x m\n",
    "    # W has the nk first elements\n",
    "    # H has the kp last elements\n",
    "    n, p = M.shape\n",
    "   \n",
    "    k = W_H.shape[0] // (n + p)\n",
    "    W = W_H[:n * k].reshape((n, k))\n",
    "    H = W_H[n * k:].reshape((k, p))\n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0, S, H0 = svds(M, 3)\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "n, k = W0.shape\n",
    "H0 = np.maximum(0,(H0.T * np.sqrt(S)).T)\n",
    "p = H0.shape[1]\n",
    "vector0 = vectorize(W0, H0)\n",
    "\n",
    "print(\"n = \" + str(n) + \"\\nk = \" + str(k) + \"\\np = \" + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(W0)  = 0.008909251527016207\n"
     ]
    }
   ],
   "source": [
    "def g(W_H):\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    return np.linalg.norm(M - np.dot(W, H))/(2*M.size)\n",
    "print(\"g(W0)  = \" + str(g(vector0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [2.0830002 , 2.0739896 , 2.0807538 , ..., 0.10482615, 0.09662902,\n",
       "        0.32288226],\n",
       "       [3.522834  , 3.5211167 , 3.5350463 , ..., 3.045007  , 3.0107825 ,\n",
       "        2.981665  ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_g_b(W_H):\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    gradient_W = -(1/M.size)*np.dot(M - np.dot(W, H), H.T)\n",
    "    return gradient_W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp.optimize.check_grad(g_function, gradient_g, vector0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\gamma >0$. Let's prove that $prox_{\\gamma \\tau \\mathbb {R}_{+}}$ is the projection onto ${\\mathbb {R_{+}}}$\n",
    "\n",
    "$\\forall x \\in \\mathbb {R}$ we can say that $x = x_{+} + x_{-}$ with $x_{+} \\in \\mathbb {R}_{+}, x_{-} \\in \\mathbb {R}_{-}$. If $x>0$, then $x_{-}=0$. If $x<0$, then $x_{+}=0$\n",
    "\n",
    "\n",
    "$prox_{\\gamma \\tau \\mathbb {R}_{+}} = argmin \\tau (y) + \\dfrac{1}{2\\gamma} \\|y-x\\|^{2}$\n",
    "\n",
    "$prox_{\\gamma \\tau \\mathbb {R}_{+}} = argmin \\tau (y) + \\dfrac{1}{2\\gamma} \\|y - x_{+} + x_{-}\\|^{2}$\n",
    "\n",
    "We want to minimize $\\|y - x_{+} + x_{-}\\|^{2}$. If $x_{+} \\geq x_{-}$ then $x \\in \\mathbb {R}_{+}$ and $y = x_{+} + x_{-}=x$\n",
    "\n",
    "If $x_{+} \\leq x_{-}$ then $x \\in \\mathbb {R}_{-*}$ and $y = x_{+} = 0$\n",
    "\n",
    "Finally we can say that $prox_{\\gamma \\tau \\mathbb {R}_{+}}$ is the projection onto ${\\mathbb {R_{+}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246380.5\n"
     ]
    }
   ],
   "source": [
    "gamma = np.linalg.norm(np.dot(H0.T, H0))\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def projected_gradient_method(val_g, grad_g, W0_H0, gamma, N):\n",
    "    W0, H0 = unvectorize_M(W0_H0, M)\n",
    "    count = 0\n",
    "    xk = W0\n",
    "    while count < N:\n",
    "        xk = np.maximum(0, xk - gamma*grad_g(vectorize(xk, H0)))\n",
    "        count += 1\n",
    "    return xk, val_g(vectorize(xk, H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.636662    0.18920523 26.735384  ]\n",
      " [ 1.7368478   0.         28.348639  ]\n",
      " [ 1.2631546   0.         30.60054   ]\n",
      " ...\n",
      " [ 0.          9.769409   23.40776   ]\n",
      " [ 0.         11.4472885  23.750692  ]\n",
      " [ 0.         14.107467   22.776457  ]] 0.008761269705636161\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "Wmin_1, g_Wmin_1 = projected_gradient_method(g, gradient_g_b, vector0, 2, 100)\n",
    "\n",
    "end = time.time()\n",
    "exe_time_1 = end - start\n",
    "print(Wmin_1, g_Wmin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic refinement for the problem with H0 fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get rid of the Lipschitz constant value, we perform a line search. We want to find a new step at each iteration. This step is given by $\\gamma _{k} = arg min f(x_{k}-\\gamma \\nabla f(x_{k}))$\n",
    "It is difficult to get a argmin over R+. We just create a list with enought possible values for gamma and search through the list the argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_line_search_gamma(gamma, W_H):\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    return g(vectorize(W - gamma*gradient_g_b(W_H), H))\n",
    "\n",
    "\n",
    "#We built a list with gamma values to search for the minimum. We try to select as many values as possible to have a great step\n",
    "def line_search(function, gradient_function, W0_H0, N):\n",
    "    W0, H0 = unvectorize_M(W0_H0, M)\n",
    "    count = 0\n",
    "    xk = W0\n",
    "    gamma_list = np.linspace(0.000001, 10, 10)\n",
    "    while count < N:\n",
    "        xk_vector = vectorize(xk, H0)\n",
    "        gggggggggggggg = np.vectorize(g_line_search_gamma, excluded=['W_H'])\n",
    "        gamma_list_g = gggggggggggggg(gamma_list, W_H=xk_vector)\n",
    "        index = np.argmin(gamma_list_g)\n",
    "        gamma = gamma_list[index] #Value of the step\n",
    "        xk = np.maximum(0, xk - gamma*gradient_function(xk_vector))\n",
    "        count += 1\n",
    "    return xk, g(vectorize(xk, H0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.160194   0.9432766 26.250122 ]\n",
      " [ 0.         0.        28.538973 ]\n",
      " [ 4.512787   0.        30.253246 ]\n",
      " ...\n",
      " [ 0.        13.786078  22.822712 ]\n",
      " [ 0.        15.991316  23.088829 ]\n",
      " [ 0.        19.37718   22.008894 ]] 0.008692754899492915\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "Wmin_2, g_Wmin_2 = line_search(g, gradient_g_b, vector0, 100)\n",
    "\n",
    "end = time.time()\n",
    "exe_time_2 = end - start\n",
    "print(Wmin_2, g_Wmin_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time : \n",
      "Projected gradient method = 3.576655626296997\n",
      "Line search method : 81.55647802352905\n",
      "W value : \n",
      "Projected gradient method = [[12.636662    0.18920523 26.735384  ]\n",
      " [ 1.7368478   0.         28.348639  ]\n",
      " [ 1.2631546   0.         30.60054   ]\n",
      " ...\n",
      " [ 0.          9.769409   23.40776   ]\n",
      " [ 0.         11.4472885  23.750692  ]\n",
      " [ 0.         14.107467   22.776457  ]]\n",
      "Line search method : [[16.160194   0.9432766 26.250122 ]\n",
      " [ 0.         0.        28.538973 ]\n",
      " [ 4.512787   0.        30.253246 ]\n",
      " ...\n",
      " [ 0.        13.786078  22.822712 ]\n",
      " [ 0.        15.991316  23.088829 ]\n",
      " [ 0.        19.37718   22.008894 ]]\n",
      "g(W) value : \n",
      "Projected gradient method = 0.008761269705636161\n",
      "Line search method : 0.008692754899492915\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution time : \\nProjected gradient method = \" + str(exe_time_1) + \"\\nLine search method : \" + str(exe_time_2))\n",
    "\n",
    "print(\"W value : \\nProjected gradient method = \" + str(Wmin_1) + \"\\nLine search method : \" + str(Wmin_2))\n",
    "\n",
    "print(\"g(W) value : \\nProjected gradient method = \" + str(g_Wmin_1) + \"\\nLine search method : \" + str(g_Wmin_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line search is longer because of the gamma search. We are search the argmin on a list with length 10000.\n",
    "\n",
    "Finally, the value found with the projected gradient method is more precise, with 100 iterations (it doesn't stop before in both case, I've checked it) it is less than line search value. this is because the step is not optimized. With a Lipschitz function, we have the best step we can find. Here we are calculating gamma by hand. The final value isn't very suprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution of the full problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W_H):\n",
    "    #print(\"W_H dans f = \" + str(W_H))\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    return np.linalg.norm(M - np.dot(W, H))/(2*M.size)\n",
    "\n",
    "def grad_f(W_H):\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    gradient_W = -(1/M.size)*np.dot(M - np.dot(W, H), H.T)\n",
    "    gradient_H = -(1/M.size)*np.dot(W.T, M - np.dot(W, H))\n",
    "    return vectorize(gradient_W, gradient_H)\n",
    "    \n",
    "\n",
    "def g_line_search_gammaf(gamma, W_H):\n",
    "    #print(\"W_H dans f = \" + str(W_H))\n",
    "    return f(W_H - gamma*grad_f(W_H))\n",
    "\n",
    "\n",
    "def line_search_final(function, gradient_function, W0, H0, N):\n",
    "    count = 0\n",
    "    xk = vectorize(W0, H0)\n",
    "    #print(\"W_H dans line search avant boucle\")\n",
    "    gamma_list = np.linspace(0.000001, 100, 10)\n",
    "    while count < N:\n",
    "        gamma_list_g = np.vectorize(g_line_search_gammaf, excluded=['W_H'])\n",
    "        #print(\"W_H dans line search boucle = \" + str(xk))\n",
    "        gamma_list_gg = gamma_list_g(gamma_list, W_H=xk)\n",
    "        index = np.argmin(gamma_list_gg)\n",
    "        gamma = gamma_list[index] #Value of the step\n",
    "        xk = np.maximum(0, xk - gamma*gradient_function(xk))\n",
    "        count += 1\n",
    "    return xk, function(xk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_H minimum = [14.212811   2.0744386 26.849216  ...  3.0367348  2.9991405  2.9226599]\n",
      "Minimum of the function (1) = 0.008519378093458851\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "W_Hfinal_projected, ffinal_projected = line_search_final(f, grad_f, W0, H0, 10)\n",
    "\n",
    "end = time.time()\n",
    "exe_time_projected = end-start\n",
    "\n",
    "print(\"W_H minimum = \" + str(W_Hfinal_projected))\n",
    "print(\"Minimum of the function (1) = \" + str(ffinal_projected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show that the value of the objective is decreasing at each iteration. This is equivalent to show that $\\forall t \\in \\mathbb {R}, \\|M-W_{t}H_{t}\\|^{2} \\leq \\|M-W_{t-1}H_{t-1}\\|^{2}$\n",
    "\n",
    "$arg min \\|M-WH_{t-1}\\|^{2} \\leq \\|M-W_{t-1}H_{t-1}\\|^{2}$\n",
    "\n",
    "$\\|M-W_{t}H_{t-1}\\|^{2} \\leq \\|M-W_{t-1}H_{t-1}\\|^{2}$\n",
    "\n",
    "$ arg min \\|M-W_{t}H\\|^{2} \\leq \\|M-W_{t}H_{t-1}\\|^{2}$\n",
    "\n",
    "$\\|M-W_{t}H_{t}\\|^{2} \\leq \\|M-W_{t}H_{t-1}\\|^{2}$\n",
    "\n",
    "Finally: $\\forall t \\in \\mathbb {R}, \\|M-W_{t}H_{t}\\|^{2} \\leq \\|M-W_{t-1}H_{t-1}\\|^{2}$\n",
    "\n",
    "Let $h : t \\longrightarrow \\dfrac{1}{2np}\\|M-W_{t}H_{t}\\|^{2}$ $h$ is decreasing and bounded so h converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_g_b_H(W_H):\n",
    "    W, H = unvectorize_M(W_H, M)\n",
    "    gradient_H = -(1/M.size)*np.dot(W.T, M - np.dot(W, H))\n",
    "    return gradient_H\n",
    "\n",
    "def projected_gradient_method_H(val_g, grad_g, W0_H0, gamma, N):\n",
    "    W0, H0 = unvectorize_M(W0_H0, M)\n",
    "    count = 0\n",
    "    xk = H0\n",
    "    while count < N:\n",
    "        xk = np.maximum(0, xk - gamma*grad_g(vectorize(W0, xk)))\n",
    "        count += 1\n",
    "    return xk, val_g(vectorize(W0, xk))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def alternate_minimization(H0_W0, N):\n",
    "    Wt, Ht = unvectorize_M(H0_W0, M)\n",
    "    \n",
    "    for n in range(N):\n",
    "        Wt = projected_gradient_method(g, gradient_g_b, vector0, 2, 100)[0]\n",
    "        Wt_Ht = vectorize(Wt, Ht)\n",
    "        Ht = projected_gradient_method_H(g, gradient_g_b_H, Wt_Ht, 2, 100)[0]\n",
    "    \n",
    "    return vectorize(Wt, Ht), g(vectorize(Wt, Ht))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "W_Hfinal_alternate, ffinal_alternate = alternate_minimization(vector0, 100)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "exe_time_alternate = end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Gradient : \n",
      "W_Hfinal = [ 11.26682281   2.32865405  25.95561028 ...,   3.03673744   2.99914265\n",
      "   2.92266226]\\Function value = 0.00851928426612\n",
      "Execution time = 4.971767902374268\n"
     ]
    }
   ],
   "source": [
    "# Comparison \n",
    "\n",
    "print(\"Projected Gradient : \\nW_Hfinal = \" + str(W_Hfinal_projected) + \n",
    "      \"\\Function value = \" + str(ffinal_projected) + \n",
    "      \"\\nExecution time = \" + str(exe_time_projected))\n",
    "\n",
    "print(\"Alternate Minimizations : \\nW_Hfinal = \" + str(W_Hfinal_alternate) + \n",
    "      \"\\Function value = \" + str(ffinal_alternate) + \n",
    "      \"\\nExecution time = \" + str(exe_time_alternate)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have stop thanks to a threshold : we know that for $W_H* = arg min f(W, H)$, then $\\nabla f(W_H*) = 0$. So if we calculate the gradient for each $x_{k}$ and say that : $\\|\\nabla f(x_{k})\\| < \\epsilon$ with $\\epsilon$ small enough, it is a criterion to stop the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
